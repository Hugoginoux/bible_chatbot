{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6b60ee",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e72722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:1 in the beginning god created the heaven and the earth.\\r\\n\\r\\n1:2 and the earth was without form, and void; and darkness was upon\\r\\nthe face of the deep. and the spirit of god moved upon the face of the\\r\\nwaters.\\r\\n\\r\\n1:3 and god said, let there be light'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = open('bible.txt', 'rb').read().decode(encoding='utf-8').lower()\n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3995a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:1 in the beginning god created the heaven and the earth.  1:2 and the earth was without form, and void; and darkness was upon the face of the deep. and the spirit of god moved upon the face of the waters.  1:3 and god said, let there be light: and '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \\n and \\r\n",
    "text = text.replace('\\r', '').replace('\\n', ' ')\n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cc479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the beginning god created the heaven and the earth. and the earth was without form, and void; and darkness was upon the face of the deep. and the spirit of god moved upon the face of the waters. and god said, let there be light: and there was ligh'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remover the paragaphs numbers\n",
    "import re\n",
    "pattern = r'[0-9]+:[0-9]+'\n",
    "text = re.sub(pattern, '', text)[1:].replace('   ', ' ').replace('  ', ' ')\n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73dff16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god said let there be light and there was light and g'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctutation\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952dba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 500000\n",
      "Unique Tokens: 10071\n",
      "Total Sequences: 499979\n",
      "This is the first sequence: in the beginning god created the heaven and the earth and the earth was without form and void and darkness was\n",
      "This is the second sequence: the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon\n",
      "This is the third sequence: beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split(' ')\n",
    "tokens = tokens[:500_000]\n",
    "\n",
    "number_of_unique_tokens = len(set(tokens))\n",
    "\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % number_of_unique_tokens)\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "# organize into sequences of tokens of input words plus one output word\n",
    "length = sequence_length + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "\n",
    "print ('Total Sequences: %d' % len(sequences))\n",
    "print ('This is the first sequence: {0}'.format(sequences[0]))\n",
    "print ('This is the second sequence: {0}'.format(sequences[1]))\n",
    "print ('This is the third sequence: {0}'.format(sequences[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3fc9c",
   "metadata": {},
   "source": [
    "## 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d330ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Embedding\n",
    " \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "# remove sequences with not enough words\n",
    "sequences = [sequences[i] for i in range(len(sequences)) if len(sequences[i])==length]\n",
    "\n",
    "vocab_size = number_of_unique_tokens + 1\n",
    " \n",
    "sequences0 = np.array(sequences)\n",
    "X, y = sequences0[:,:-1], sequences0[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f128529b",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# The learned embedding needs to know how many dimensions will be used to represent each word. \n",
    "# That is, the size of the embedding vector space. That is, the size of the embedding vector space.\n",
    "# Common values are 50, 100, and 300. Consider testing smaller or larger values.\n",
    "dimensions_to_represent_word = 100\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
    "# We will use a two LSTM hidden layers with 100 memory cells each. \n",
    "# More memory cells and a deeper network may achieve better results.\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training may take a few hours on modern hardware without GPUs. \n",
    "# You can speed it up with a larger batch size and/or fewer training epochs.\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=128, \n",
    "    epochs=50, \n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "model.save('bible_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25ec58",
   "metadata": {},
   "source": [
    "## 3. Predict word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : \n",
      "and the lord said unto moses and aaron go get thee down for thy people i will make of thee a great nation the twelve children of\n",
      "next words : \n",
      "israel which thou hast brought forth from the hand of the lord and the lord spake unto moses saying speak unto the children of israel and say unto them ye shall not be eaten and they shall not be ashamed and they shall be as the thing that is in the land of egypt and the lord said unto moses stretch out thine hand and the angel of the lord was kindled against israel and he said unto him i pray thee to the king of israel and say unto him behold i will not go down to the children\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.models.load_model('bible_model', compile=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# must be longer than sequence_length\n",
    "seed = \"and the lord said unto moses and aaron go get thee down for thy people i will make of thee a great nation the twelve children of\"\n",
    "n_predictions = 100\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "print('seed : \\n' + seed)\n",
    "last_words = seed.split(' ')[-sequence_length:]\n",
    "preds = []\n",
    "\n",
    "for i in range(n_predictions):\n",
    "    example = tokenizer.texts_to_sequences([last_words])\n",
    "    prediction = model.predict(np.array(example), verbose=0)\n",
    "    predicted_word = np.argmax(prediction)\n",
    "    last_words = last_words[1:]\n",
    "    last_words.append(reverse_word_map[predicted_word])\n",
    "    preds.append(reverse_word_map[predicted_word])\n",
    "    \n",
    "print('next words : \\n' + ' '.join(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
