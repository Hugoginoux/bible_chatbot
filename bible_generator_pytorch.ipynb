{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c82c8e",
   "metadata": {},
   "source": [
    "# Generating Bible\n",
    "\n",
    "We are going to create two generating models with LSTM: at character and word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ab5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23391244",
   "metadata": {},
   "source": [
    "### Define classes for datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d61d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text chunk at index idx.\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        # Return (x, y) where x has length 40 and y has length 40.\n",
    "        # y should be x shifted by 1 time.\n",
    "        return (text_chunk[:-1], text_chunk[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6951110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        # Set to an embedding layer of vocab_size by embed_dim.\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim\n",
    "        ) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        # LSTM layer: recurrent network with long memory\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Make a linear layer from rnn_hidden_size to vocab_size.\n",
    "        # This will be used to get the yt for each xt.\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden=None, cell=None):\n",
    "        # Get the embeddings for text.\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # Pass out, hidden and cell through the rnn.\n",
    "        # If hidden is None, don't specify it and just use out.\n",
    "        if hidden is not None:\n",
    "            out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        else:\n",
    "            out, (hidden, cell) = out, (None, None)\n",
    "        \n",
    "        # Pass out through fc.\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize to zeros of 1 by batch_size. Maybe Xavier initialization?\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060566a",
   "metadata": {},
   "source": [
    "## I At character level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0320b",
   "metadata": {},
   "source": [
    "### Get the data and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5611de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:1 In the beginning God created the heaven and the earth.\\n\\n1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep. And the Spirit of God moved upon the face of the\\nwaters.\\n\\n1:3 And God said, Let there be light: and '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bible.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e64a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 4332453\n",
      "Unique Characters: 74\n"
     ]
    }
   ],
   "source": [
    "# Get the unique set of characters.\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76393bdb",
   "metadata": {},
   "source": [
    "### Tokenize and get other helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a445114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (4332453,)\n",
      "1:1 In the begi      == Encoding ==>  [11 20 11  1 31 61  1 67 55 52  1 49 52 54 56]\n",
      "[61 61 56 61 54  1 29 62 51  1 50 65 52 48 67]  == Reverse  ==>  nning God creat\n"
     ]
    }
   ],
   "source": [
    "# The universe of characters.\n",
    "chars_sorted = sorted(char_set)\n",
    "\n",
    "# Effectively, these maps are the tokenizer.\n",
    "# Map each char to a unique int. This is a dict.\n",
    "char2int = {char: i for i, char in enumerate(chars_sorted)}\n",
    "# Do the revverse of the above, this should be a np array.\n",
    "int2char = np.array(chars_sorted)\n",
    "\n",
    "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
    "text_encoded = np.array([char2int[c] for c in text])\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:30], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:30]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c418ca0",
   "metadata": {},
   "source": [
    "### Process the data and get the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f429dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "# Break up the data into chunks of size 41. This should be a list of lists.\n",
    "# Use text_encoded. This will be used to get (x, y) pairs.\n",
    "text_chunks = np.array([list(text_encoded[i:i+chunk_size]) for i in range(len(text_encoded)-chunk_size)])\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71328555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): '1:1 In the beginning God created the hea'\n",
      "Target (y): ':1 In the beginning God created the heav'\n",
      "\n",
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): ':1 In the beginning God created the heav'\n",
      "Target (y): '1 In the beginning God created the heave'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y is x shifted by 1\n",
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    # 40 characters for source and target ...\n",
    "    print(seq.shape, target.shape)\n",
    "    print('Input (x):', repr(''.join(int2char[seq])))\n",
    "    print('Target (y):', repr(''.join(int2char[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb989c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00789dfd",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33380607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(74, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=74, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2char)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)\n",
    "\n",
    "char_model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "char_model = char_model.to(device)\n",
    "char_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521b8db",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f47f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3080\n",
      "Epoch 100 loss: 1.6483\n",
      "Epoch 200 loss: 1.4469\n",
      "Epoch 300 loss: 1.4253\n",
      "Epoch 400 loss: 1.3399\n",
      "Epoch 500 loss: 1.2940\n",
      "Epoch 600 loss: 1.2677\n",
      "Epoch 700 loss: 1.3161\n",
      "Epoch 800 loss: 1.2719\n",
      "Epoch 900 loss: 1.2610\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(char_model.parameters(), lr=0.005)\n",
    "\n",
    "# Set to 1000.\n",
    "num_epochs = 1000\n",
    "\n",
    "# epochs here will mean batches.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = char_model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get the next batch from seq_dl\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    \n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    # Pass through the model.\n",
    "    logits, _ = char_model(seq_batch, hidden=hidden, cell=cell)\n",
    "    \n",
    "    # Get the loss.\n",
    "    # You'll need to reshape / view things to make this work.\n",
    "    #target_ = torch.Tensor([[[1 if k==target_batch[i, j] else 0 for k in range(vocab_size)] for j in range(seq_length)] for i in range(batch_size)])\n",
    "    loss += criterion(input=logits.transpose(1,2), target=target_batch.long())   \n",
    "    \n",
    "    # Do back prop.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Do an optimization step.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Get the value in the tensor loss.\n",
    "    loss = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547467d",
   "metadata": {},
   "source": [
    "### Random decoding : Sample each character according to the proba distribution given by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614fb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp/ipykernel_23352/3662168778.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  m = Categorical(nn.functional.softmax(logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:4 And the Lord said unto Moses, My servants idol, and Hamah,\n",
      "underadgeth, and in\n",
      "Jerusaleaz.\n",
      "\n",
      "9:14 He done the cannot shall be with ointment, and Hozor and said, Go of\n",
      "NebrethKseph and Aaron, and two life is\n",
      "ne, Go yet heart.\n",
      "\n",
      "38:37 And the children of Isroah,\n",
      "and tell doctcrine beproys the priests:\n",
      "therewing, nor mitting of silves are trulf was divest tree? which shut on his\n",
      "fasters: 30:8 She into Egypt,: the name\n",
      "on his time is iniquity.\n",
      "\n",
      "9:32 And it did not the manness with\n",
      "the ground against her.\n",
      "\n",
      "44:20 And the I have given them and I will do it live; changed I am the unterices and walking over the\n",
      "Phechehioh, and have build, he was rise to the\n",
      "LORD; and understand het an oil, les spake eleven and hations men, she shall she\n",
      "that man laughters, and come for anger hand, and\n",
      "the people, and wilt passeth to this\n",
      "prey and all the names, against the hosseh, O gland cutcity acceptanting her.\n",
      "\n",
      "47:23 And the law is a mother waiteth, and and\n",
      "full be bullock, nor habitation; who disple to\n",
      "be numbered his cloudy: 4:13 I pr\n"
     ]
    }
   ],
   "source": [
    "def random_sample_char(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=500, \n",
    "):\n",
    "\n",
    "    # Encode starting string into a tensor using char2str.\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    \n",
    "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
    "    encoded_input = encoded_input.reshape(1, len(starting_str))\n",
    "\n",
    "    # This will be what you generate, but it starts off with something.\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for c in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[0, c].reshape(1,1)\n",
    "        # Pass out through, note we update hidden and cell and use them again\n",
    "        _, (hidden, cell) = model(out, hidden=hidden, cell=cell)\n",
    "        \n",
    "    \n",
    "    # Gte the last char; note we did not do go to the last char above.\n",
    "    last_char = starting_str[-1]\n",
    "    # Generate chars one at a time, add them to generated_str.\n",
    "    # Do this over and over until you get the desired length.\n",
    "    for i in range(len_generated_text):\n",
    "        \n",
    "        # Use hidden and cell from the above.\n",
    "        # Use last_char, which will be updated over and over.\n",
    "        logits, (hidden, cell) = model(torch.as_tensor([[char2int[last_char]]]), hidden=hidden, cell=cell) \n",
    "        \n",
    "        # Get the logits.\n",
    "        logits = logits[0][0]\n",
    "        \n",
    "        # m is a random variable with probabilities based on the softmax of the logits.\n",
    "        m = Categorical(nn.functional.softmax(logits))\n",
    "        \n",
    "        # Generate from m 1 char.\n",
    "        last_char_index = m.sample().item()\n",
    "        last_char = int2char[last_char_index]\n",
    "        \n",
    "        # Add the geenrated char to generated_str, but pass it through int2str so that \n",
    "        generated_str += last_char\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "char_model.to(device)\n",
    "print(random_sample_char(char_model, starting_str='23:4 And the Lord said unto Moses', len_generated_text=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc391e",
   "metadata": {},
   "source": [
    "## II At word level\n",
    "\n",
    "### Get data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20064042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god said let there be light and there was light and g'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bible.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "# remove \\n and \\r and lower case\n",
    "text = text.replace('\\r', '').replace('\\n', ' ').lower()\n",
    "\n",
    "# remover the paragaphs numbers\n",
    "import re\n",
    "pattern = r'[0-9]+:[0-9]+'\n",
    "text = re.sub(pattern, '', text)[1:].replace('   ', ' ').replace('  ', ' ')\n",
    "\n",
    "# remove punctutation\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0daf6ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 790010\n",
      "Unique Words: 12697\n"
     ]
    }
   ],
   "source": [
    "# Get the unique set of words.\n",
    "text = text.split()\n",
    "word_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Words:', len(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6658d39",
   "metadata": {},
   "source": [
    "### Tokenize and get helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "206dead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (790010,)\n",
      "in the beginning god created      == Encoding ==>  [ 5723 11307  1205  4787  2657]\n",
      "[11307  5258   532 11307  3439]  == Reverse  ==>  the heaven and the earth\n"
     ]
    }
   ],
   "source": [
    "# The universe of words.\n",
    "words_sorted = sorted(word_set)\n",
    "\n",
    "# Effectively, these maps are the tokenizer.\n",
    "# Map each char to a unique int. This is a dict.\n",
    "w2int = {w: i for i, w in enumerate(words_sorted)}\n",
    "# Do the revverse of the above, this should be a np array.\n",
    "int2w = np.array(words_sorted)\n",
    "\n",
    "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
    "text_encoded = np.array([w2int[w] for w in text])\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(' '.join(text[:5]), '     == Encoding ==> ', text_encoded[:5])\n",
    "print(text_encoded[5:10], ' == Reverse  ==> ', ' '.join(int2w[text_encoded[5:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392be23",
   "metadata": {},
   "source": [
    "### Process the data and get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4b4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "# Break up the data into chunks of size 41. This should be a list of lists.\n",
    "# Use text_encoded. This will be used to get (x, y) pairs.\n",
    "text_chunks = np.array([list(text_encoded[i:i+chunk_size]) for i in range(len(text_encoded)-chunk_size)])\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e8cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and'\n",
      "Target (y): 'the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god'\n",
      "\n",
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god'\n",
      "Target (y): 'beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god said'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    # 40 characters for source and target ...\n",
    "    print(seq.shape, target.shape)\n",
    "    print('Input (x):', repr(' '.join(int2w[seq])))\n",
    "    print('Target (y):', repr(' '.join(int2w[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59f03208",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d40f6",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85efe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(12697, 512)\n",
       "  (rnn): LSTM(512, 2048, batch_first=True)\n",
       "  (fc): Linear(in_features=2048, out_features=12697, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2w)\n",
    "embed_dim = 512\n",
    "rnn_hidden_size = 2048\n",
    "torch.manual_seed(1)\n",
    "\n",
    "word_model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "word_model = word_model.to(device)\n",
    "word_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba83d5",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5080fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 9.4491\n",
      "Epoch 100 loss: 5.0692\n",
      "Epoch 200 loss: 4.4230\n",
      "Epoch 300 loss: 3.9703\n",
      "Epoch 400 loss: 3.8680\n",
      "Epoch 500 loss: 3.6103\n",
      "Epoch 600 loss: 3.1783\n",
      "Epoch 700 loss: 3.2783\n",
      "Epoch 800 loss: 2.9378\n",
      "Epoch 900 loss: 2.9400\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(word_model.parameters(), lr=0.005)\n",
    "\n",
    "# Set to 500.\n",
    "num_epochs = 1000\n",
    "\n",
    "# epochs here will mean batches.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = word_model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get the next batch from seq_dl\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    \n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    # Pass through the model.\n",
    "    logits, _ = word_model(seq_batch, hidden=hidden, cell=cell)\n",
    "    \n",
    "    # Get the loss.\n",
    "    # You'll need to reshape / view things to make this work.\n",
    "    #target_ = torch.Tensor([[[1 if k==target_batch[i, j] else 0 for k in range(vocab_size)] for j in range(seq_length)] for i in range(batch_size)])\n",
    "    loss += criterion(input=logits.transpose(1,2), target=target_batch.long())   \n",
    "    \n",
    "    # Do back prop.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Do an optimization step.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Get the value in the tensor loss.\n",
    "    loss = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfe43e",
   "metadata": {},
   "source": [
    "### Random decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62961786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp/ipykernel_23352/1151685456.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  m = Categorical(nn.functional.softmax(logits))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the lord said unto moses the people after that paul had appeared unto him and moses said unto aaron take a censer and put fire therein and when the trumpet of sin is the crown of man wherefore evil in my ways work in righteousness and in the house of god with all his heart and with all his host against the lord his maker is mine and i am bereaved of my spots therefore were the princes rebuked the congregation of the lord hath no meat therefore my fury shall devour flesh and the dead bodies of the righteousness of the increase he shall\n"
     ]
    }
   ],
   "source": [
    "def random_sample_word(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=100, \n",
    "):\n",
    "\n",
    "    starting_str = starting_str.split()\n",
    "    \n",
    "    # Encode starting string into a tensor using char2str.\n",
    "    encoded_input = torch.tensor([w2int[w] for w in starting_str])\n",
    "    \n",
    "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
    "    encoded_input = encoded_input.reshape(1, len(starting_str))\n",
    "\n",
    "    # This will be what you generate, but it starts off with something.\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for c in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[0, c].reshape(1,1)\n",
    "        # Pass out through, note we update hidden and cell and use them again\n",
    "        _, (hidden, cell) = model(out, hidden=hidden, cell=cell)\n",
    "        \n",
    "    \n",
    "    # Gte the last word; note we did not do go to the last word above.\n",
    "    last_word = starting_str[-1]\n",
    "    # Generate chars one at a time, add them to generated_str.\n",
    "    # Do this over and over until you get the desired length.\n",
    "    for i in range(len_generated_text):\n",
    "        \n",
    "        # Use hidden and cell from the above.\n",
    "        # Use last_word, which will be updated over and over.\n",
    "        logits, (hidden, cell) = model(torch.as_tensor([[w2int[last_word]]]), hidden=hidden, cell=cell) \n",
    "        \n",
    "        # Get the logits.\n",
    "        logits = logits[0][0]\n",
    "        \n",
    "        # m is a random variable with probabilities based on the softmax of the logits.\n",
    "        m = Categorical(nn.functional.softmax(logits))\n",
    "        \n",
    "        # Generate from m 1 char.\n",
    "        last_word_index = m.sample().item()\n",
    "        last_word = int2w[last_word_index]\n",
    "        \n",
    "        # Add the geenrated char to generated_str \n",
    "        generated_str.append(last_word)\n",
    "        \n",
    "    return ' '.join(generated_str)\n",
    "\n",
    "word_model.to(device)\n",
    "print(random_sample_word(word_model, starting_str='and the lord said unto moses', len_generated_text=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef9d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a588b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
